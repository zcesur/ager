aes(x = Age, y = Department, color = Age) +
geom_point() +
ggtitle("Average age by departments in 2015-16") +
theme(text = element_text(family = "", face = "plain",
colour = "black", size = 12,
hjust = 0.5, vjust = 0.5, angle = 0,
lineheight = 0.9),
axis.text.y = element_text(hjust = 1), angle = 10) +
scale_color_gradient(low = "yellow", high = "red")
ggplot(department_average_df) +
aes(x = Age, y = Department, color = Age) +
geom_point() +
ggtitle("Average age by departments in 2015-16") +
theme(text = element_text(family = "", face = "plain",
colour = "black", size = 12,
hjust = 0.5, vjust = 0.5, angle = 0,
lineheight = 0.9),
axis.text.y = element_text(hjust = 1, angle = 10)) +
scale_color_gradient(low = "yellow", high = "red")
ggplot(department_average_df) +
aes(x = Age, y = Department, color = Age) +
geom_point() +
ggtitle("Average age by departments in 2015-16") +
theme(axis.text.y = element_text(hjust = 1, angle = 5)) +
scale_color_gradient(low = "yellow", high = "red")
ggplot(department_average_df) +
aes(x = Age, y = Department, color = Age) +
geom_point() +
ggtitle("Average age by departments in 2015-16") +
theme(axis.text.y = element_text(hjust = 10, angle = 5)) +
scale_color_gradient(low = "yellow", high = "red")
ggplot(department_average_df) +
aes(x = Age, y = Department, color = x) +
geom_point() +
ggtitle("Average age by departments in 2015-16") +
theme(axis.text.y = element_text(color = x)) +
scale_color_gradient(low = "yellow", high = "red")
ggplot(department_average_df) +
aes(x = Age, y = Department) +
geom_point(aes(color = Age)) +
ggtitle("Average age by departments in 2015-16") +
scale_color_gradient(low = "yellow", high = "red")
ggplot(department_average_df) +
aes(x = Age, y = Department) +
geom_point(aes(color = Age, size = Age)) +
ggtitle("Average age by departments in 2015-16") +
scale_color_gradient(low = "yellow", high = "red")
render("vignette.Rmd", "pdf_document")
render("./vignettes/vignette.Rmd", "pdf_document")
rm(list=ls())
library(devtools)
library(magrittr)
library(ager)
library(knitr)
library(roxygen2)
library(rvest)
library(ggplot2)
library(dplyr)
library(reshape2)
load_all()
render("./vignettes/vignette.Rmd", "pdf_document")
render("./vignettes/vignette.Rmd", "pdf_document")
render("./vignettes/vignette.Rmd", "pdf_document")
rm(list=ls())
render("./vignettes/vignette.Rmd", "pdf_document")
document()
library(ager)
library(ager)
document()
library(ager)
devtools::create()
library(devtools)
devtools::create()
document()
?collect_departments
library(ager)
library(ager)
library(ager)
document()
library(ager)
library(ager)
?collect_departments
render("./vignettes/vignette.Rmd", "html_document")
rm(list=ls())
render("./vignettes/vignette.Rmd", "html_document")
#' Clean catalog files
#'
#' \code{scrub_catalog} removes all irrelevant data in catalog files.
#'
#' @param year an academic year of which faculty data is desired.
#' @return The output is a character vector that contains one record per element.
#' @examples
#' scrub_catalog("2011-12")
scrub_catalog <- function(year){
# Open up a txt file and store lines of information as character strings of the
# vector 'data'
flat_file <- paste(year, ".txt", sep = "") %>%
system.file("extdata", ., package = "ager") %>%
readLines(skipNul = TRUE)
# Remove leftovers from the previous page.
relevant_line <- grep("FACULTY", flat_file)[1]
flat_file <- flat_file[-seq(1, relevant_line - 1)]
# Remove all other irrelevant data at the beginning of the document until coming
# across a line containing a comma
relevant_line <- grep(",", flat_file)[1]
flat_file <- flat_file[-seq(1, relevant_line - 1)]
# Remove the strings which are of length 0 and those containing page numbers. 4 is not
# chosen arbitrarily but rather based on other years' documents. For instance in the
# previous 2 years, the strings that contain pages are formatted as 3-digit integer and
# a space.
flat_file <- setdiff(flat_file, flat_file[nchar(flat_file) <= 4])
if (year == "2012-13"){
# Clean up text if it is formatted with hard line endings
short_lines <- order(nchar(flat_file), decreasing=FALSE)[1:15]
long_lines <- order(nchar(flat_file), decreasing=TRUE)[1:15]
# Evaluate which of the long lines are immediately followed by a short line
cut_lines <- short_lines[(short_lines - 1) %in% long_lines]
# Join the cut off line with the line above
flat_file[cut_lines - 1] <- paste(flat_file[cut_lines - 1], flat_file[cut_lines], sep=" ")
# Remove cut off lines
flat_file <- setdiff(flat_file, flat_file[cut_lines])
}
return(flat_file)
}
rm(list=ls())
flat_file <- scrub_catalog("2012-2013")
flat_file <- scrub_catalog("2012-13")
names <- collect_names(flat_file)
head(flat_file)
short_lines <- order(nchar(flat_file), decreasing=FALSE)[1:15]
long_lines <- order(nchar(flat_file), decreasing=TRUE)[1:15]
cut_lines <- short_lines[(short_lines - 1) %in% long_lines]
flat_file[cut_lines - 1] <- paste(flat_file[cut_lines - 1], flat_file[cut_lines], sep=" ")
flat_file <- setdiff(flat_file, flat_file[cut_lines])
flat_file <- scrub_catalog("2012-13")
year <- "2012-13"
year == "2012-13"
flat_file <- paste(year, ".txt", sep = "") %>%
system.file("extdata", ., package = "ager") %>%
readLines(skipNul = TRUE)
relevant_line <- grep("FACULTY", flat_file)[1]
flat_file <- flat_file[-seq(1, relevant_line - 1)]
relevant_line <- grep(",", flat_file)[1]
flat_file <- flat_file[-seq(1, relevant_line - 1)]
flat_file <- setdiff(flat_file, flat_file[nchar(flat_file) <= 4])
if (year == "2012-13"){
short_lines <- order(nchar(flat_file), decreasing=FALSE)[1:15]
long_lines <- order(nchar(flat_file), decreasing=TRUE)[1:15]
cut_lines <- short_lines[(short_lines - 1) %in% long_lines]
}
flat_file[cut_lines]
long_lines
names <- collect_names(flat_file)
data_source <- flat_file
academic_data <- extract_data(names, data_source, useInternet = FALSE)
head(academic_data)
#' Extract academic data
#'
#' \code{extract_data} extracts the data that contains academic background of faculty
#' members from (1) a catalog chosen as the data source and (2) Williams' web directory.
#'
#' @param names character vector that contains the names of faculty members whose academic
#' information is desired.
#' @param data_source a plain text file that contains the desired information.
#' @param useInternet logical. If TRUE missing data is extracted from Williams' website,
#' otherwise it is left blank.
#' @return The output is a named vector that contains the graduation year and the degree of
#' faculty members.
#' @section Note:
#' Assigning the TRUE value to \code{useInternet} may result in a code execution that takes a
#' while to finish.
#' @examples
#' academic_year <- "2015-16"
#' + flat_file <- scrub_catalog(academic_year)
#' + names <- collect_names(flat_file, reformat = TRUE)
#' + data_source <- scrub_catalog("2013-14")
#' + academic_data <- extract_data(names, data_source, useInternet = TRUE)
extract_data <- function(names, data_source, useInternet = FALSE){
pattern <- "-(.*?);|\\d{4}, (B\\w+|AB), (.*?),"
academic_data <- sapply(names, function(x){
vector <-
regexpr(pattern, data_source[grep(x, data_source)]) %>%
regmatches(data_source[grep(x, data_source)], .)
if(length(vector) == 0){
return(NA)
}else{
return(vector)
}
})
# The carrot makes sure that we only get the first word of each names element.
first_names <- regmatches(names, regexpr("^\\w+", names))
# The optional \\w+[-] pattern makes sure that we take hypenated last names such as
# 'Robert Baker-White' into account.
last_names <- regmatches(names, regexpr("(\\w+[-])?\\w+$", names))
# Generate search queries in order to access data on the web directory
search_queries <- paste(first_names, "+", last_names, sep = "")
search_links <- paste("http://www.williams.edu/people/?s_directory=",
search_queries,
sep = "")
# Return the indices of the elements with missing data
missing_data <- unname(which(is.na(academic_data == TRUE)))
if (useInternet == TRUE){
for (i in missing_data){
tryCatch({
directory_search <- read_html(search_links[i])
}, error=function(e){cat("Unidentified error in the first connection during", print(i), "\n")})
email_nodes <- html_nodes(directory_search, css = ".phone+ .email a")
if (length(email_nodes) != 0){
unix_ids <- gsub("mailto:|@williams.edu", "", html_attr(email_nodes, "href"))
if (length(unix_ids) > 1){
print(cat("Warning: More than one people with the name", print(names[i]), "\n"))
}
tryCatch({
profile_data <- html_text(
html_nodes(
read_html(
paste("http://www.williams.edu/profile/", unix_ids[1], sep="")),
css = ".profile-education .profile-subsection"))
}, error=function(e){cat("Unidentified error in the second connection during", print(i), "\n")})
pattern <- "(B\\.(.*?)\\.|A\\.B\\.|Diploma) .*? \\(\\d{4}\\)"
# Catch potential exceptions
tryCatch({
academic_data[i] <- regmatches(profile_data, regexpr(pattern, profile_data))
}, error=function(e){cat("Missing information on", print(names[i]), "\n")})
}
}
}
return(academic_data)
}
academic_data <- extract_data(names, data_source, useInternet = FALSE)
#' Extract academic data
#'
#' \code{extract_data} extracts the data that contains academic background of faculty
#' members from (1) a catalog chosen as the data source and (2) Williams' web directory.
#'
#' @param names character vector that contains the names of faculty members whose academic
#' information is desired.
#' @param data_source a plain text file that contains the desired information.
#' @param useInternet logical. If TRUE missing data is extracted from Williams' website,
#' otherwise it is left blank.
#' @return The output is a named vector that contains the graduation year and the degree of
#' faculty members.
#' @section Note:
#' Assigning the TRUE value to \code{useInternet} may result in a code execution that takes a
#' while to finish.
#' @examples
#' academic_year <- "2015-16"
#' + flat_file <- scrub_catalog(academic_year)
#' + names <- collect_names(flat_file, reformat = TRUE)
#' + data_source <- scrub_catalog("2013-14")
#' + academic_data <- extract_data(names, data_source, useInternet = TRUE)
extract_data <- function(names, data_source, useInternet = FALSE){
pattern <- "(-(.*?);|\\d{4}, (B\\w+|AB), (.*?),)"
academic_data <- sapply(names, function(x){
vector <-
regexpr(pattern, data_source[grep(x, data_source)]) %>%
regmatches(data_source[grep(x, data_source)], .)
if(length(vector) == 0){
return(NA)
}else{
return(vector)
}
})
# The carrot makes sure that we only get the first word of each names element.
first_names <- regmatches(names, regexpr("^\\w+", names))
# The optional \\w+[-] pattern makes sure that we take hypenated last names such as
# 'Robert Baker-White' into account.
last_names <- regmatches(names, regexpr("(\\w+[-])?\\w+$", names))
# Generate search queries in order to access data on the web directory
search_queries <- paste(first_names, "+", last_names, sep = "")
search_links <- paste("http://www.williams.edu/people/?s_directory=",
search_queries,
sep = "")
# Return the indices of the elements with missing data
missing_data <- unname(which(is.na(academic_data == TRUE)))
if (useInternet == TRUE){
for (i in missing_data){
tryCatch({
directory_search <- read_html(search_links[i])
}, error=function(e){cat("Unidentified error in the first connection during", print(i), "\n")})
email_nodes <- html_nodes(directory_search, css = ".phone+ .email a")
if (length(email_nodes) != 0){
unix_ids <- gsub("mailto:|@williams.edu", "", html_attr(email_nodes, "href"))
if (length(unix_ids) > 1){
print(cat("Warning: More than one people with the name", print(names[i]), "\n"))
}
tryCatch({
profile_data <- html_text(
html_nodes(
read_html(
paste("http://www.williams.edu/profile/", unix_ids[1], sep="")),
css = ".profile-education .profile-subsection"))
}, error=function(e){cat("Unidentified error in the second connection during", print(i), "\n")})
pattern <- "(B\\.(.*?)\\.|A\\.B\\.|Diploma) .*? \\(\\d{4}\\)"
# Catch potential exceptions
tryCatch({
academic_data[i] <- regmatches(profile_data, regexpr(pattern, profile_data))
}, error=function(e){cat("Missing information on", print(names[i]), "\n")})
}
}
}
return(academic_data)
}
academic_data <- extract_data(names, data_source, useInternet = FALSE)
#' Extract academic data
#'
#' \code{extract_data} extracts the data that contains academic background of faculty
#' members from (1) a catalog chosen as the data source and (2) Williams' web directory.
#'
#' @param names character vector that contains the names of faculty members whose academic
#' information is desired.
#' @param data_source a plain text file that contains the desired information.
#' @param useInternet logical. If TRUE missing data is extracted from Williams' website,
#' otherwise it is left blank.
#' @return The output is a named vector that contains the graduation year and the degree of
#' faculty members.
#' @section Note:
#' Assigning the TRUE value to \code{useInternet} may result in a code execution that takes a
#' while to finish.
#' @examples
#' academic_year <- "2015-16"
#' + flat_file <- scrub_catalog(academic_year)
#' + names <- collect_names(flat_file, reformat = TRUE)
#' + data_source <- scrub_catalog("2013-14")
#' + academic_data <- extract_data(names, data_source, useInternet = TRUE)
extract_data <- function(names, data_source, useInternet = FALSE){
pattern <- "-(.*?);"
academic_data <- sapply(names, function(x){
vector <-
regexpr(pattern, data_source[grep(x, data_source)]) %>%
regmatches(data_source[grep(x, data_source)], .)
if(length(vector) == 0){
return(NA)
}else{
return(vector)
}
})
# The carrot makes sure that we only get the first word of each names element.
first_names <- regmatches(names, regexpr("^\\w+", names))
# The optional \\w+[-] pattern makes sure that we take hypenated last names such as
# 'Robert Baker-White' into account.
last_names <- regmatches(names, regexpr("(\\w+[-])?\\w+$", names))
# Generate search queries in order to access data on the web directory
search_queries <- paste(first_names, "+", last_names, sep = "")
search_links <- paste("http://www.williams.edu/people/?s_directory=",
search_queries,
sep = "")
# Return the indices of the elements with missing data
missing_data <- unname(which(is.na(academic_data == TRUE)))
if (useInternet == TRUE){
for (i in missing_data){
tryCatch({
directory_search <- read_html(search_links[i])
}, error=function(e){cat("Unidentified error in the first connection during", print(i), "\n")})
email_nodes <- html_nodes(directory_search, css = ".phone+ .email a")
if (length(email_nodes) != 0){
unix_ids <- gsub("mailto:|@williams.edu", "", html_attr(email_nodes, "href"))
if (length(unix_ids) > 1){
print(cat("Warning: More than one people with the name", print(names[i]), "\n"))
}
tryCatch({
profile_data <- html_text(
html_nodes(
read_html(
paste("http://www.williams.edu/profile/", unix_ids[1], sep="")),
css = ".profile-education .profile-subsection"))
}, error=function(e){cat("Unidentified error in the second connection during", print(i), "\n")})
pattern <- "(B\\.(.*?)\\.|A\\.B\\.|Diploma) .*? \\(\\d{4}\\)"
# Catch potential exceptions
tryCatch({
academic_data[i] <- regmatches(profile_data, regexpr(pattern, profile_data))
}, error=function(e){cat("Missing information on", print(names[i]), "\n")})
}
}
}
return(academic_data)
}
academic_data <- extract_data(names, data_source, useInternet = FALSE)
#' Extract academic data
#'
#' \code{extract_data} extracts the data that contains academic background of faculty
#' members from (1) a catalog chosen as the data source and (2) Williams' web directory.
#'
#' @param names character vector that contains the names of faculty members whose academic
#' information is desired.
#' @param data_source a plain text file that contains the desired information.
#' @param useInternet logical. If TRUE missing data is extracted from Williams' website,
#' otherwise it is left blank.
#' @return The output is a named vector that contains the graduation year and the degree of
#' faculty members.
#' @section Note:
#' Assigning the TRUE value to \code{useInternet} may result in a code execution that takes a
#' while to finish.
#' @examples
#' academic_year <- "2015-16"
#' + flat_file <- scrub_catalog(academic_year)
#' + names <- collect_names(flat_file, reformat = TRUE)
#' + data_source <- scrub_catalog("2013-14")
#' + academic_data <- extract_data(names, data_source, useInternet = TRUE)
extract_data <- function(names, data_source, useInternet = FALSE){
pattern <- "-(.*?);"
academic_data <- sapply(names, function(x){
vector <-
regexpr(pattern, data_source[grep(x, data_source)]) %>%
regmatches(data_source[grep(x, data_source)], .)
if(length(vector) == 0){
return(NA)
}else{
return(vector)
}
})
if (useInternet == TRUE){
# The carrot makes sure that we only get the first word of each names element.
first_names <- regmatches(names, regexpr("^\\w+", names))
# The optional \\w+[-] pattern makes sure that we take hypenated last names such as
# 'Robert Baker-White' into account.
last_names <- regmatches(names, regexpr("(\\w+[-])?\\w+$", names))
# Generate search queries in order to access data on the web directory
search_queries <- paste(first_names, "+", last_names, sep = "")
search_links <- paste("http://www.williams.edu/people/?s_directory=",
search_queries,
sep = "")
# Return the indices of the elements with missing data
missing_data <- unname(which(is.na(academic_data == TRUE)))
for (i in missing_data){
tryCatch({
directory_search <- read_html(search_links[i])
}, error=function(e){cat("Unidentified error in the first connection during", print(i), "\n")})
email_nodes <- html_nodes(directory_search, css = ".phone+ .email a")
if (length(email_nodes) != 0){
unix_ids <- gsub("mailto:|@williams.edu", "", html_attr(email_nodes, "href"))
if (length(unix_ids) > 1){
print(cat("Warning: More than one people with the name", print(names[i]), "\n"))
}
tryCatch({
profile_data <- html_text(
html_nodes(
read_html(
paste("http://www.williams.edu/profile/", unix_ids[1], sep="")),
css = ".profile-education .profile-subsection"))
}, error=function(e){cat("Unidentified error in the second connection during", print(i), "\n")})
pattern <- "(B\\.(.*?)\\.|A\\.B\\.|Diploma) .*? \\(\\d{4}\\)"
# Catch potential exceptions
tryCatch({
academic_data[i] <- regmatches(profile_data, regexpr(pattern, profile_data))
}, error=function(e){cat("Missing information on", print(names[i]), "\n")})
}
}
}
return(academic_data)
}
academic_data <- extract_data(names, data_source, useInternet = FALSE)
academic_data[[1]]
academic_data[1
]
names[1]
pattern <- "-(.*?);"
regexpr(pattern, data_source[grep(names[i], data_source)])
regexpr(pattern, data_source[grep(names[1], data_source)])
grep(names[i], data_source)
grep(names[1], data_source)
regexpr(pattern, 1)
regexpr(pattern, data_source[1])
data_source[1]
head(data_source)
flat_file <- scrub_catalog("2013-14")
names <- collect_names(flat_file)
rm(list=ls())
library(ager)
library(ager)
library(ager)
devtools::install_github("zcesur/ager")
lıbrary(ager)
?ager
devtools::check()
library(ager)
rvest::%>%
?reshape2
library(reshape2)
?reshape2
??reshape2
library(ager)
devtools::install_github("Yuanchu/creditr")
?read_html
?"%>%"
?"."
?magrittr::"."
rm(list=ls())
?magrittr::"."
?magrittr::"%>%"
?magrittr::%>%
?"%>%"
dplyr
?dplyr
?plyr
?mutate
?"."
?"%>%"
source('C:/Users/Zafer/Desktop/R Project/packages/ager/R/transform_data.R')
library(ager)
library(ager)
library(ager)
rm(list=ls())
library(ager)
document()
?"%>%"
library(ager)
library(ager)
Sys.which(Sys.getenv("R_QPDF", "qpdf"))
Sys.getenv("PATH")
library(ager)
library(ager)
